{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/taraprole/cityscapes-segmentation?scriptVersionId=88975331\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\n!pip install keras-unet\nfrom keras_unet.models import vanilla_unet\n\n\nfrom keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.utils import img_to_array\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import CategoricalCrossentropy, binary_crossentropy, SparseCategoricalCrossentropy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-28T20:19:17.700959Z","iopub.execute_input":"2022-02-28T20:19:17.701227Z","iopub.status.idle":"2022-02-28T20:19:24.879088Z","shell.execute_reply.started":"2022-02-28T20:19:17.701196Z","shell.execute_reply":"2022-02-28T20:19:24.878257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport glob\nfrom IPython.display import Image, display\n\ninput_dir = sorted(glob.glob('../input/cityscapes-dataset/leftImg8bit_trainvaltest/leftImg8bit/train/*/*.png', recursive=True)) #TRAINING\ntarget_dir = sorted(glob.glob('../input/cityscapes-dataset/gtFine_trainvaltest/gtFine/train/**/*_gtFine_color.png', recursive=True)) #TRAINING ANSWERS\n\n\nprint(\"Number of samples:\", len(input_dir))\n\nimgSize = (1024, 2048)\nclasses = 30\nbatchSize = 2\n\ncityscapesDataset = zip(input_dir, target_dir)\nprint(\"Length of dataset:\", len(list(cityscapesDataset)))\n\n# for input_path, target_path in zip(input_dir, target_dir):\n#     print(input_path, \"|\", target_path)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:19:24.882961Z","iopub.execute_input":"2022-02-28T20:19:24.883191Z","iopub.status.idle":"2022-02-28T20:19:26.014899Z","shell.execute_reply.started":"2022-02-28T20:19:24.883163Z","shell.execute_reply":"2022-02-28T20:19:26.014171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image, display\nfrom tensorflow.keras.preprocessing.image import load_img\nimport PIL\nfrom PIL import ImageOps\nimport numpy\nfrom numpy import ndarray\nfrom numpy import asarray\n\n\n\n\ndef rgb_to_onehot(rgb_arr, color_dict):\n    num_classes = len(color_dict)\n    shape = rgb_arr.shape[:2]+(num_classes,)\n    arr = np.zeros( shape, dtype=np.int8 )\n    for i, cls in enumerate(color_dict):\n        arr[:,:,i] = np.all(rgb_arr.reshape( (-1,3) ) == color_dict[i], axis=1).reshape(shape[:2])\n    return arr\n\ndef onehot_to_rgb(onehot, color_dict):\n    single_layer = np.argmax(onehot, axis=-1)\n    output = np.zeros( onehot.shape[:2]+(3,) )\n    for k in color_dict.keys():\n        output[single_layer==k] = color_dict[k]\n    return np.uint8(output)\n\n# Display input image #7\ndisplay(Image(filename=input_dir[9]))\n\n# Display auto-contrast version of corresponding target (per-pixel categories)\nimg = PIL.ImageOps.grayscale(load_img(target_dir[9]))\ndisplay(img)\n\nimgAsArray = img_to_array(img)\nshapeOfArray = imgAsArray.shape\nprint(shapeOfArray)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:19:26.016592Z","iopub.execute_input":"2022-02-28T20:19:26.016998Z","iopub.status.idle":"2022-02-28T20:19:26.17598Z","shell.execute_reply.started":"2022-02-28T20:19:26.016959Z","shell.execute_reply":"2022-02-28T20:19:26.175204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Cityscapes(keras.utils.Sequence):\n    #iterate over the data as numpy arrays\n    \n    def __init__(self, batchSize, imgSize, input_dir, target_dir):\n        self.batchSize = batchSize\n        self.imgSize = imgSize\n        self.input_dir = input_dir\n        self.target_dir = target_dir\n    \n    def __len__(self):\n        return len(self.target_dir) // self.batchSize\n    \n    def __getitem__(self, index):\n        i = index * self.batchSize\n        batch_input_img_paths = self.input_dir[i:i+self.batchSize]\n        batch_target_img_paths = self.target_dir[i:i+self.batchSize]\n        imgArray = np.zeros((self.batchSize,) + self.imgSize + (3,), dtype='float32')\n        \n        for j, path in enumerate(batch_input_img_paths):\n            img = load_img(path, target_size = self.imgSize)\n            img = img.resize((2048,1024))\n            imgArray[j] = img\n            \n        maskArray = np.zeros((self.batchSize,) + self.imgSize, dtype='float32')\n        \n        for j, path in enumerate(batch_target_img_paths):\n            img = load_img(path, target_size=self.imgSize)\n            img = PIL.ImageOps.grayscale(img)\n            img = img.resize((2048,1024))\n            maskArray[j] = img\n        \n        return imgArray, maskArray","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:19:26.177158Z","iopub.execute_input":"2022-02-28T20:19:26.177612Z","iopub.status.idle":"2022-02-28T20:19:26.188064Z","shell.execute_reply.started":"2022-02-28T20:19:26.177572Z","shell.execute_reply":"2022-02-28T20:19:26.187316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_input_img_paths = sorted(glob.glob('../input/cityscapes-dataset/leftImg8bit_trainvaltest/leftImg8bit/val/*/*.png', recursive=True))\nval_target_img_paths = sorted(glob.glob('../input/cityscapes-dataset/gtFine_trainvaltest/gtFine/val/*/*_gtFine_color.png', recursive=True))\ntest_input_img_paths = sorted(glob.glob('../input/cityscapes-dataset/leftImg8bit_trainvaltest/leftImg8bit/test/*/*.png', recursive=True))\ntest_target_img_paths = sorted(glob.glob('../input/cityscapes-dataset/gtFine_trainvaltest/gtFine/test/*/*_gtFine_color.png', recursive=True))\n\n# Instantiate data Sequences for each split\ntrain_gen = Cityscapes(batchSize, imgSize, input_dir, target_dir)\nval_gen = Cityscapes(batchSize, imgSize, val_input_img_paths, val_target_img_paths)\ntest_gen = Cityscapes(batchSize, imgSize, test_input_img_paths, test_target_img_paths)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:19:26.191015Z","iopub.execute_input":"2022-02-28T20:19:26.191325Z","iopub.status.idle":"2022-02-28T20:19:26.238467Z","shell.execute_reply.started":"2022-02-28T20:19:26.191288Z","shell.execute_reply":"2022-02-28T20:19:26.237811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dice coefficient loss function- credit to Soriba D. on Medium for providing this loss function\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return -dice_coef(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:19:26.240419Z","iopub.execute_input":"2022-02-28T20:19:26.240892Z","iopub.status.idle":"2022-02-28T20:19:26.246867Z","shell.execute_reply.started":"2022-02-28T20:19:26.240856Z","shell.execute_reply":"2022-02-28T20:19:26.246104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Implementation of the UNet architecture by soribadiaby on Github\n\n# def getUNet():\n#     inputs = Input(shape=(1024, 2048, 3))\n#     conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n#     conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n#     pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n#     conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n#     conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n#     pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n#     conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n#     conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n#     pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n#     conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n#     conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n#     pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\n#     conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n#     conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n\n#     up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n#     conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n#     conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n\n#     up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n#     conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n#     conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n\n#     up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n#     conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n#     conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n\n#     up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n#     conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n#     conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n\n#     conv10 = Conv2D(1, (1, 1), activation='softmax')(conv9)\n\n#     model = Model(inputs=[inputs], outputs=[conv10])\n\n# #     model.compile(optimizer=Adam(learning_rate=1e-3), loss=binary_crossentropy, metrics=[dice_coef])\n\n#     return model\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:19:26.247887Z","iopub.execute_input":"2022-02-28T20:19:26.249916Z","iopub.status.idle":"2022-02-28T20:19:26.256866Z","shell.execute_reply.started":"2022-02-28T20:19:26.249889Z","shell.execute_reply":"2022-02-28T20:19:26.256089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = getUNet()\nmodel = vanilla_unet(input_shape=(512, 512, 3))\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:19:26.258255Z","iopub.execute_input":"2022-02-28T20:19:26.258937Z","iopub.status.idle":"2022-02-28T20:19:26.482916Z","shell.execute_reply.started":"2022-02-28T20:19:26.258893Z","shell.execute_reply":"2022-02-28T20:19:26.48222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configure the model for training.\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss=CategoricalCrossentropy())\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"cityscapes_segmentation.h5\", save_best_only=True)\n]\n\n\nmodel.fit(train_gen, epochs=10, validation_data=val_gen, callbacks=callbacks)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:19:26.484146Z","iopub.execute_input":"2022-02-28T20:19:26.484381Z","iopub.status.idle":"2022-02-28T20:19:41.963334Z","shell.execute_reply.started":"2022-02-28T20:19:26.484347Z","shell.execute_reply":"2022-02-28T20:19:41.9624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = model.predict(test_gen)\n\ndef display_mask(i):\n    \"\"\"Quick utility to display a model's prediction.\"\"\"\n    mask = np.argmax(test_preds[i], axis=-1)\n    mask = np.expand_dims(mask, axis=-1)\n    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n    display(img)\n\n\n# Display results for validation image #10\ni = 10\n\n# Display input image\ndisplay(Image(filename=test_input_img_paths[i]))\n\n# Display ground-truth target mask\nimg = PIL.ImageOps.autocontrast(load_img(test_target_img_paths[i]))\ndisplay(img)\n\n# Display mask predicted by our model\ndisplay_mask(i)  # Note that the model only sees inputs at 150x150.","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:19:41.964587Z","iopub.status.idle":"2022-02-28T20:19:41.965191Z","shell.execute_reply.started":"2022-02-28T20:19:41.964936Z","shell.execute_reply":"2022-02-28T20:19:41.964964Z"},"trusted":true},"execution_count":null,"outputs":[]}]}